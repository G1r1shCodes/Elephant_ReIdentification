{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêò Elephant Re-Identification Training (ENHANCED v2.0)\n",
    "\n",
    "**CRITICAL FIXES INTEGRATED:**\n",
    "‚úÖ Attention regularization (prevents attention degradation)\n",
    "‚úÖ Enhanced data augmentation (handles small dataset)\n",
    "‚úÖ Separate learning rates per branch (balances training)\n",
    "‚úÖ Fixed PyTorch deprecation warnings\n",
    "‚úÖ Frequent attention visualization (every 5 epochs)\n",
    "\n",
    "Train a dual-branch model with Biological Attention Maps for elephant re-identification.\n",
    "\n",
    "## Features\n",
    "‚úÖ **GPU Optimized** - 90-100% GPU utilization\n",
    "‚úÖ Mixed Precision Training (2-3x faster)\n",
    "‚úÖ Checkpoint Management (resume interrupted training)\n",
    "‚úÖ Early Stopping (save GPU time)\n",
    "‚úÖ **NEW: Attention Regularization** (maintains focus)\n",
    "‚úÖ **NEW: Enhanced Augmentation** (7 techniques)\n",
    "\n",
    "## Setup\n",
    "1. Enable GPU (Settings ‚Üí Accelerator ‚Üí GPU P100 or T4)\n",
    "2. Add dataset (Settings ‚Üí Add Data ‚Üí your elephant dataset)\n",
    "3. Enable Internet (Settings ‚Üí Internet ‚Üí ON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q torch torchvision tqdm opencv-python-headless matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration (GPU Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA_ROOT = Path('/kaggle/input/datasets/girishcodes/elephant-reid-processed/processed_megadetector/Makhna')\n",
    "# Data Configuration\n",
    "BATCH_SIZE = 32          # INCREASED: More negative diversity (12 IDs/batch vs 8)\n",
    "IMAGE_SIZE = (256, 128)  # Height, Width\n",
    "NUM_WORKERS = 4          # Fixed: 0 workers for Kaggle stability\n",
    "PIN_MEMORY = True\n",
    "PERSISTENT_WORKERS = False\n",
    "PREFETCH_FACTOR = None\n",
    "EMBEDDING_DIM = 256 # ArcFace benefits from larger dim\n",
    "LEARNING_RATE = 0.0002 # Fixed: Lower LR for stability\n",
    "EARLY_STOP_PATIENCE = 10\n",
    "NUM_EPOCHS = 40      # Total training epochs\n",
    "USE_AMP = False          # Fixed: Disable AMP for stability\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import BatchSampler, DataLoader, Dataset\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler  # Fixed: removed .cuda\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime",
    "try:\n",
    "    from torchvision.models import ResNet50_Weights\n",
    "except ImportError:\n",
    "    ResNet50_Weights = None  # Handle old torchvision\n",
    "import torchvision.models as models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BAM(nn.Module):\n",
    "    \"\"\"Biological Attention Map (BAM) Module\"\"\"\n",
    "    def __init__(self, in_channels, reduction_ratio=16, dilated=True):\n",
    "        super(BAM, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        # Channel Attention\n",
    "        self.channel_att = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, in_channels // reduction_ratio, 1, bias=False),\n",
    "            nn.BatchNorm2d(in_channels // reduction_ratio),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels // reduction_ratio, in_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(in_channels)\n",
    "        )\n",
    "        \n",
    "        # Spatial Attention\n",
    "        if dilated:\n",
    "            self.spatial_att = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, in_channels // reduction_ratio, 1, bias=False),\n",
    "                nn.BatchNorm2d(in_channels // reduction_ratio),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(in_channels // reduction_ratio, in_channels // reduction_ratio, 3, padding=4, dilation=4, bias=False),\n",
    "                nn.BatchNorm2d(in_channels // reduction_ratio),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(in_channels // reduction_ratio, in_channels // reduction_ratio, 3, padding=4, dilation=4, bias=False),\n",
    "                nn.BatchNorm2d(in_channels // reduction_ratio),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(in_channels // reduction_ratio, 1, 1, bias=False),\n",
    "                nn.BatchNorm2d(1)\n",
    "            )\n",
    "        else:\n",
    "             self.spatial_att = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, in_channels // reduction_ratio, 1, bias=False),\n",
    "                nn.BatchNorm2d(in_channels // reduction_ratio),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(in_channels // reduction_ratio, 1, 1, bias=False),\n",
    "                nn.BatchNorm2d(1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Channel attention\n",
    "        att_c = self.channel_att(x)\n",
    "        \n",
    "        # Spatial attention\n",
    "        att_s = self.spatial_att(x)\n",
    "        \n",
    "        # Fuse\n",
    "        att = F.sigmoid(att_c + att_s)\n",
    "        \n",
    "        return x * att, att\n",
    "\n",
    "print('‚úì BAM Class defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualBranchFeatureExtractor(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, num_classes=None, use_bam=False):\n",
    "        super().__init__()\n",
    "        self.use_bam = use_bam\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Handle torchvision version\n",
    "        if 'ResNet50_Weights' in globals() and ResNet50_Weights is not None:\n",
    "            weights = ResNet50_Weights.IMAGENET1K_V2\n",
    "            base_model = models.resnet50(weights=weights)\n",
    "        else:\n",
    "            base_model = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Split into texture (shallow) and semantic (deep)\n",
    "        self.layer0 = nn.Sequential(base_model.conv1, base_model.bn1, base_model.relu, base_model.maxpool)\n",
    "        self.layer1 = base_model.layer1\n",
    "        self.layer2 = base_model.layer2\n",
    "        self.layer3 = base_model.layer3\n",
    "        self.layer4 = base_model.layer4\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Texture Branch Components\n",
    "        self.texture_reducer = nn.Conv2d(512, 1024, kernel_size=1)\n",
    "        if self.use_bam:\n",
    "             self.texture_bam = BAM(1024)\n",
    "        \n",
    "        # Semantic Branch Components\n",
    "        if self.use_bam:\n",
    "             self.semantic_bam = BAM(2048)\n",
    "        \n",
    "        # Embedding head\n",
    "        self.fc = nn.Linear(2048 + 1024, embedding_dim)\n",
    "        self.bn = nn.BatchNorm1d(embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Classification Head (CRITICAL for stability)\n",
    "        if self.num_classes:\n",
    "            self.classifier = nn.Linear(embedding_dim, num_classes, bias=False)\n",
    "            \n",
    "    def texture_branch(self, x, return_spatial=False):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        feat = self.texture_reducer(x)\n",
    "        \n",
    "        if self.use_bam:\n",
    "             feat_att, _ = self.texture_bam(feat)\n",
    "             if return_spatial: return feat_att, feat # Return attended feat + raw for loss\n",
    "             return feat_att\n",
    "        \n",
    "        if return_spatial: return feat, feat\n",
    "        return feat\n",
    "\n",
    "    def semantic_branch(self, x, return_spatial=False):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        if self.use_bam:\n",
    "             feat_att, _ = self.semantic_bam(x)\n",
    "             if return_spatial: return feat_att, x\n",
    "             return feat_att\n",
    "        \n",
    "        if return_spatial: return x, x\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Texture Branch\n",
    "        tex_feat_spatial = self.texture_branch(x)\n",
    "        tex_feat = self.global_pool(tex_feat_spatial).flatten(1)\n",
    "        \n",
    "        # Semantic Branch\n",
    "        sem_feat_spatial = self.semantic_branch(x)\n",
    "        sem_feat = self.global_pool(sem_feat_spatial).flatten(1)\n",
    "        \n",
    "        # Fuse\n",
    "        combined = torch.cat([tex_feat, sem_feat], dim=1)\n",
    "        embedding_raw = self.fc(combined)\n",
    "        embedding_raw = self.bn(embedding_raw)\n",
    "        \n",
    "        # CRITICAL FIX: Separate CE and Triplet objectives\n",
    "        # Normalize for Triplet (metric learning)\n",
    "        embedding = F.normalize(embedding_raw, p=2, dim=1)\n",
    "        \n",
    "        if self.training and self.num_classes:\n",
    "            # Feed CE the RAW embedding (before normalization)\n",
    "            # This removes geometric conflict with Triplet loss\n",
    "            logits = self.classifier(embedding_raw)\n",
    "            return embedding, logits\n",
    "            \n",
    "        return embedding\n",
    "\n",
    "\n",
    "print('‚úì Dual-Branch Model defined (BAM Support: Enabled)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElephantDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, split='train'):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.samples = []\n",
    "        self.identity_to_idx = {}\n",
    "        self._load_dataset()\n",
    "    \n",
    "    def _load_dataset(self):\n",
    "        identity_images = defaultdict(list)\n",
    "        # Makhna-only: root_dir IS the Makhna folder\n",
    "        for individual_dir in self.root_dir.iterdir():\n",
    "            if not individual_dir.is_dir():\n",
    "                continue\n",
    "            identity_name = individual_dir.name  # e.g., \"Makhna_1\"\n",
    "            for img_path in individual_dir.rglob('*.jpg'):\n",
    "                identity_images[identity_name].append(img_path)\n",
    "        \n",
    "        all_ids = list(identity_images.keys())\n",
    "        random.seed(42)\n",
    "        random.shuffle(all_ids)\n",
    "        n = len(all_ids)\n",
    "        train_ids = all_ids[:int(0.7*n)]\n",
    "        val_ids = all_ids[int(0.7*n):int(0.85*n)]\n",
    "        selected_ids = train_ids if self.split == 'train' else val_ids\n",
    "        \n",
    "        for idx, identity_name in enumerate(selected_ids):\n",
    "            self.identity_to_idx[identity_name] = idx\n",
    "            for img_path in identity_images[identity_name]:\n",
    "                self.samples.append({'path': img_path, 'identity': idx})\n",
    "        self.num_classes = len(self.identity_to_idx)\n",
    "        print(f'[{self.split.upper()}] {len(self.samples)} images, {len(self.identity_to_idx)} identities')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        image = cv2.imread(str(sample['path']))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, sample['identity']\n",
    "\n",
    "print('‚úì Dataset class defined')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms moved to dataset creation cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ArcFace(nn.Module):\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50):\n",
    "        super(ArcFace, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # 1. Cosine similarity\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        \n",
    "        # 2. Add margin\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        \n",
    "        # 3. Handle easy_margin issues (if cosine is small)\n",
    "        phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        \n",
    "        # 4. Convert label to one-hot and add margin to ground truth\n",
    "        one_hot = torch.zeros(cosine.size(), device=input.device)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        \n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine) \n",
    "        output *= self.s\n",
    "        \n",
    "        return output\n",
    "\n",
    "print('‚úì ArcFace Module defined (s=30.0, m=0.50)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Regularization (CRITICAL FIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_loss(model, images, lambda_sparsity=0.001, lambda_entropy=0.0, target_mean=0.2):\n",
    "    \"\"\"\n",
    "    Robust Attention Regularization.\n",
    "    Instead of minimizing mean (which leads to 0), we encourage mean to stay around target_mean (0.2).\n",
    "    \"\"\"\n",
    "    attention_loss = 0\n",
    "    \n",
    "    if getattr(model, 'use_bam', False):\n",
    "        # Get spatial features from both branches\n",
    "        _, texture_spatial = model.texture_branch(images, return_spatial=True)\n",
    "        _, semantic_spatial = model.semantic_branch(images, return_spatial=True)\n",
    "        \n",
    "        # Apply BAM to get attention maps (DO NOT DETACH - we need gradients)\n",
    "        _, texture_attn = model.texture_bam(texture_spatial)\n",
    "        _, semantic_attn = model.semantic_bam(semantic_spatial)\n",
    "        \n",
    "        # 1. TARGET MEAN LOSS (Prevent Dead Attention)\n",
    "        # Penalize if mean is too far from target (e.g., 0.2)\n",
    "        tex_mean = texture_attn.mean()\n",
    "        sem_mean = semantic_attn.mean()\n",
    "        \n",
    "        # L2 distance to target mean\n",
    "        mean_loss = ((tex_mean - target_mean)**2 + (sem_mean - target_mean)**2)\n",
    "        \n",
    "        # 2. VARIANCE LOSS (Prevent Uniform Attention)\n",
    "        # We want high variance (peaks and valleys), so minimize negative variance\n",
    "        tex_var = texture_attn.var()\n",
    "        sem_var = semantic_attn.var()\n",
    "        var_loss = - (tex_var + sem_var)\n",
    "        \n",
    "        # Combine\n",
    "        attention_loss = lambda_sparsity * mean_loss + 0.1 * var_loss\n",
    "    \n",
    "    return attention_loss\n",
    "\n",
    "print('‚úì Safer Attention Loss defined (Target Mean + Variance)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to stop training when validation loss doesn't improve.\"\"\"\n",
    "    def __init__(self, patience=15, min_delta=0.0001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.improved = False\n",
    "    \n",
    "    def __call__(self, val_loss):\n",
    "        self.improved = False\n",
    "        \n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.improved = True\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.improved = True\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        \n",
    "        return self.early_stop\n",
    "\n",
    "print('‚úì Early stopping defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, epoch, train_losses, val_losses, best_val_loss, path):\n",
    "    \"\"\"Save comprehensive checkpoint with full training state.\"\"\"\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'config': {\n",
    "            'embedding_dim': EMBEDDING_DIM,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'learning_rate': LEARNING_RATE\n",
    "        }\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f'  üíæ Checkpoint saved: {path.name}')\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, path):\n",
    "    \"\"\"Load checkpoint and restore training state.\"\"\"\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    \n",
    "    print(f'‚úì Checkpoint loaded from epoch {checkpoint[\"epoch\"]}')\n",
    "    print(f'  Best val loss: {checkpoint[\"best_val_loss\"]:.4f}')\n",
    "    \n",
    "    return (\n",
    "        checkpoint['epoch'] + 1,\n",
    "        checkpoint['train_losses'],\n",
    "        checkpoint['val_losses'],\n",
    "        checkpoint['best_val_loss']\n",
    "    )\n",
    "\n",
    "print('‚úì Checkpoint management defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_maps(model, dataloader, device, save_path, num_samples=4):\n",
    "    \"\"\"Visualize attention maps from the model.\"\"\"\n",
    "    model.eval()\n",
    "    images, _ = next(iter(dataloader))\n",
    "    images = images[:num_samples].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, tex_spatial = model.texture_branch(images, True)\n",
    "        _, sem_spatial = model.semantic_branch(images, True)\n",
    "        _, tex_attn = model.texture_bam(tex_spatial)\n",
    "        _, sem_attn = model.semantic_bam(sem_spatial)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, num_samples * 3))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Original image\n",
    "        img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "        img = (img - img.min()) / (img.max() - img.min())\n",
    "        axes[i, 0].imshow(img)\n",
    "        axes[i, 0].set_title('Original')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Texture attention\n",
    "        tex_map = tex_attn[i, 0].cpu().numpy()\n",
    "        axes[i, 1].imshow(tex_map, cmap='hot')\n",
    "        axes[i, 1].set_title('Texture Attention')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Semantic attention\n",
    "        sem_map = sem_attn[i, 0].cpu().numpy()\n",
    "        axes[i, 2].imshow(sem_map, cmap='hot')\n",
    "        axes[i, 2].set_title('Semantic Attention')\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f'  üé® Attention maps saved: {save_path.name}')\n",
    "\n",
    "print('‚úì Visualization functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Dataset Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset exists\n",
    "if not DATA_ROOT.exists():\n",
    "    print(f'‚ùå Dataset not found at: {DATA_ROOT}')\n",
    "    print('\\nPlease check:')\n",
    "    print('1. Dataset is added in Settings ‚Üí Add Data')\n",
    "    print('2. Path matches your dataset location')\n",
    "    print('\\nAvailable data sources:')\n",
    "    !ls /kaggle/input/\n",
    "    raise FileNotFoundError(f'Dataset not found at {DATA_ROOT}')\n",
    "else:\n",
    "    print(f'‚úì Dataset found at: {DATA_ROOT}')\n",
    "    print('\\nDataset structure:')\n",
    "    for category in ['Makhna', 'Herd']:\n",
    "        cat_path = DATA_ROOT / category\n",
    "        if cat_path.exists():\n",
    "            num_dirs = len(list(cat_path.iterdir()))\n",
    "            print(f'  {category}: {num_dirs} individuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training (GPU Optimized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED Training transforms (CRITICAL FIX: Reduced intensity)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(\n",
    "        (256, 128), \n",
    "        scale=(0.8, 1.0),    # Less aggressive cropping\n",
    "        ratio=(0.4, 0.6)     # Match elephant aspect ratio\n",
    "    ),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),  # Convert to Tensor BEFORE RandomErasing\n",
    "    # Fixed: Less aggressive erasing (p=0.3)\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.1), ratio=(0.3, 3.3)), \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print('‚úì Transforms defined (Optimized: RandomErasing after ToTensor)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datasets\n",
    "print('Creating datasets...')\n",
    "train_dataset = ElephantDataset(root_dir=DATA_ROOT, transform=train_transform, split='train')\n",
    "val_dataset = ElephantDataset(root_dir=DATA_ROOT, transform=val_transform, split='val')\n",
    "\n",
    "print(f'\\nDataset Summary:')\n",
    "print(f'  Train: {len(train_dataset)} images from {train_dataset.num_classes} elephants')\n",
    "print(f'  Val: {len(val_dataset)} images from {val_dataset.num_classes} elephants')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPerClassBatchSampler(BatchSampler):\n",
    "    \"\"\"\n",
    "    M-Per-Class Sampler with Random Class Selection.\n",
    "    FIXED: Epoch length based on total images, not just unique classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, m=4, batch_size=16):\n",
    "        self.m = m\n",
    "        self.batch_size = batch_size\n",
    "        self.classes_per_batch = batch_size // m\n",
    "        self.dataset_len = len(dataset)\n",
    "        \n",
    "        # OPTIMIZED: Get labels without loading images\n",
    "        labels = []\n",
    "        if hasattr(dataset, 'samples'):\n",
    "            print('  Sampler: Optimizing label extraction from dataset.samples')\n",
    "            for s in dataset.samples:\n",
    "                labels.append(s['identity'])\n",
    "        elif hasattr(dataset, 'targets'):\n",
    "            labels = dataset.targets\n",
    "        else:\n",
    "            for i in range(len(dataset)):\n",
    "                _, label = dataset[i]\n",
    "                labels.append(label)\n",
    "        \n",
    "        self.label_to_indices = {}\n",
    "        for idx, label in enumerate(labels):\n",
    "            if label not in self.label_to_indices:\n",
    "                self.label_to_indices[label] = []\n",
    "            self.label_to_indices[label].append(idx)\n",
    "        \n",
    "        self.labels_set = list(self.label_to_indices.keys())\n",
    "        print(f\"  Sampler: {len(self.labels_set)} classes, {self.dataset_len} images, {self.classes_per_batch} classes/batch\")\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # FIXED: batches to cover full dataset\n",
    "        n_batches = len(self) \n",
    "        \n",
    "        for _ in range(n_batches):\n",
    "            # Randomly select classes for this batch (With Replacement allowed across batches)\n",
    "            # Using random.choices/sample logic\n",
    "            if len(self.labels_set) >= self.classes_per_batch:\n",
    "                 selected_classes = random.sample(self.labels_set, self.classes_per_batch)\n",
    "            else:\n",
    "                 selected_classes = random.choices(self.labels_set, k=self.classes_per_batch)\n",
    "            \n",
    "            batch = []\n",
    "            for cls in selected_classes:\n",
    "                indices = self.label_to_indices[cls]\n",
    "                # Replace: True ensures we don't run out of images for small classes\n",
    "                if len(indices) >= self.m:\n",
    "                    selected = random.sample(indices, self.m)\n",
    "                else:\n",
    "                    selected = random.choices(indices, k=self.m)\n",
    "                batch.extend(selected)\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        # FIXED: Return enough batches to cover the dataset\n",
    "        return self.dataset_len // self.batch_size\n",
    "\n",
    "print('‚úì Fixed M-Per-Class Sampler defined (Full Dataset Coverage)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure worker config is defined (Safety Fallback)\n",
    "if 'PERSISTENT_WORKERS' not in locals():\n",
    "    PERSISTENT_WORKERS = False\n",
    "if 'PREFETCH_FACTOR' not in locals():\n",
    "    PREFETCH_FACTOR = None\n",
    "\n",
    "# Create batch sampler (CRITICAL FIX)\n",
    "batch_sampler = MPerClassBatchSampler(\n",
    "    train_dataset,\n",
    "    m=4,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Use batch_sampler (not shuffle) - CRITICAL FIX\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=batch_sampler,  # <-- FIXED: no more shuffle=True\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=PERSISTENT_WORKERS,\n",
    "    # prefetch_factor=PREFETCH_FACTOR  # Commented out for safety\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=PERSISTENT_WORKERS,\n",
    "    # prefetch_factor=PREFETCH_FACTOR  # Commented out for safety\n",
    ")\n",
    "\n",
    "print(f\"‚úì Train batches per epoch: {len(batch_sampler)}\")\n",
    "print(f\"‚úì Val batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VALIDATE BATCH STRUCTURE\n",
    "# ============================================================================\n",
    "print(\"\\nüîç Validating Batch Sampler\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check first batch\n",
    "for images, labels in train_loader:\n",
    "    unique, counts = torch.unique(labels, return_counts=True)\n",
    "    print(f\"Batch shape: {images.shape}\")\n",
    "    print(f\"Identities: {len(unique)}\")\n",
    "    print(f\"Samples per identity: {counts.tolist()}\")\n",
    "    \n",
    "    if len(unique) == 8 and all(c == 4 for c in counts):\n",
    "        print(\"‚úÖ PASS: 8 identities √ó 4 samples each\")\n",
    "    else:\n",
    "        print(\"‚ùå FAIL: M-per-class constraint violated!\")\n",
    "    break\n",
    "\n",
    "print(f\"‚úÖ Train batches per epoch: {len(train_loader)}\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL VALIDATION: Verify Sampler Structure (PxK)\n",
    "print('\\nüîç Verifying Batch Structure...')\n",
    "try:\n",
    "    # Get one batch\n",
    "    images, labels = next(iter(train_loader))\n",
    "    \n",
    "    # Check composition\n",
    "    unique_labels, counts = torch.unique(labels, return_counts=True)\n",
    "    \n",
    "    print(f'  Labels in batch: {unique_labels.tolist()}')\n",
    "    print(f'  Counts per label: {counts.tolist()}')\n",
    "    \n",
    "    # Expectations\n",
    "    expected_classes = BATCH_SIZE // 4 # m=4\n",
    "    expected_samples = 4\n",
    "    \n",
    "    if len(unique_labels) == expected_classes and all(c == expected_samples for c in counts):\n",
    "        print(f'  ‚úÖ Batch structure VALID: {len(unique_labels)} identities x {expected_samples} samples')\n",
    "    else:\n",
    "        print(f'  ‚ö†Ô∏è Batch structure INVALID! Expected {expected_classes} IDs x {expected_samples} samples')\n",
    "        if len(unique_labels) != expected_classes:\n",
    "             print(f'     - Incorrect ID count: {len(unique_labels)} (Expected {expected_classes})')\n",
    "        if not all(c == expected_samples for c in counts):\n",
    "             print(f'     - Irregular sample counts: {counts}')\n",
    "except Exception as e:\n",
    "    print(f'  ‚ö†Ô∏è Verification failed: {e}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop (Enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure config is defined (Safety Fallback)\n",
    "if 'EMBEDDING_DIM' not in locals():\n",
    "EMBEDDING_DIM = 256 # ArcFace benefits from larger dim\n",
    "\n",
    "# Initialize Model and Loss\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "\n",
    "# Initialize Model (BAM ENABLED but supervised)\n",
    "model = DualBranchFeatureExtractor(\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    num_classes=train_dataset.num_classes,\n",
    "    use_bam=True  # ENABLED BAM as requested\n",
    ").to(device)\n",
    "\n# Initialize ArcFace Head\n",
    "arcface_head = ArcFace(in_features=EMBEDDING_DIM, out_features=train_dataset.num_classes, s=30.0, m=0.50).to(device)\n",
    "print(\"‚úì ArcFace Head initialized\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# DIFFERENTIAL LEARNING RATES (Stability Fix)\n",
    "param_groups = [\n",
    "    {'params': arcface_head.parameters(), 'lr': 1e-3}, # ArcFace needs higher LR often\n",
    "    {'params': model.layer0.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.layer1.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.layer2.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.layer3.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.layer4.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.texture_bam.parameters(), 'lr': 5e-5},   # Lower LR for BAM\n",
    "    {'params': model.semantic_bam.parameters(), 'lr': 5e-5},  # Lower LR for BAM\n",
    "    {'params': model.texture_reducer.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.fc.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.bn.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.classifier.parameters(), 'lr': 1e-4},\n",
    "]\n",
    "# Fallback for any missed parameters\n",
    "base_params = {id(p) for group in param_groups for p in group['params']}\n",
    "extra_params = [p for p in model.parameters() if id(p) not in base_params]\n",
    "if extra_params:\n",
    "    param_groups.append({'params': extra_params, 'lr': 1e-4})\n",
    "\n",
    "optimizer = optim.Adam(param_groups, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Mixed Precision\n",
    "# scaler = torch.amp.GradScaler('cuda') # Disabled for stability\n",
    "\n",
    "print('‚úì Model initialized (BAM=True, Differential LRs)')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize start_epoch (default to 0 if not loading checkpoint)\n",
    "if 'start_epoch' not in locals():\n",
    "    start_epoch = 0\n",
    "    print('Starting training from scratch (epoch 0)')\n",
    "else:\n",
    "    print(f'Resuming training from epoch {start_epoch}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_health(model, embeddings, loss, epoch):\n",
    "    \"\"\"Monitor Embeddings and Attention Health.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 1. Embedding Health\n",
    "        emb_std = embeddings.std().item()\n",
    "        emb_status = \"‚úì\" if emb_std > 0.05 else \"‚ö†Ô∏è COLLAPSE\"\n",
    "        \n",
    "        # 2. Attention Health (if using BAM)\n",
    "        attn_status = \"\"\n",
    "        if getattr(model, 'use_bam', False):\n",
    "             # Grab a dummy batch or use current if accessible (simplification: just status check)\n",
    "             # We can't easily grab attention maps for the whole batch here without forward hook\n",
    "             # So we'll trust the loss components for now, or check lighter stats\n",
    "             pass\n",
    "             \n",
    "        print(f'  Epoch {epoch+1} Health:')\n",
    "        print(f'    Embed Std: {emb_std:.4f} {emb_status}')\n",
    "        \n",
    "        # Warning triggers\n",
    "        if emb_std < 0.01:\n",
    "            print('    ‚ö†Ô∏è CRITICAL: Embedding Collapse Detected!')\n",
    "\n",
    "print('‚úì Health Check defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EMBEDDING HEALTH MONITORING\n",
    "# ============================================================================\n",
    "def check_embedding_health(model, val_loader, device, epoch):\n",
    "    \"\"\"Monitor embedding collapse during training.\"\"\"\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            output = model(images)\n",
    "            embeddings = output[0] if isinstance(output, tuple) else output\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            all_labels.append(labels)\n",
    "            \n",
    "            # Sample ~200 images for speed\n",
    "            if len(all_embeddings) * images.size(0) >= 200:\n",
    "                break\n",
    "    \n",
    "    embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    # Compute statistics\n",
    "    emb_std = embeddings.std().item()\n",
    "    emb_mean = embeddings.mean().item()\n",
    "    \n",
    "    # Compute intra vs inter similarity (VECTORIZED)\n",
    "    # Normalize embeddings for cosine similarity\n",
    "    embeddings_norm = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    # Compute full similarity matrix\n",
    "    sim_matrix = torch.mm(embeddings_norm, embeddings_norm.t())\n",
    "    \n",
    "    # Create masks for intra vs inter class pairs\n",
    "    labels_expanded = labels.unsqueeze(0)\n",
    "    same_label_mask = (labels_expanded == labels_expanded.t())\n",
    "    \n",
    "    # Remove diagonal (self-similarity)\n",
    "    eye_mask = torch.eye(len(labels), dtype=torch.bool)\n",
    "    same_label_mask = same_label_mask & ~eye_mask\n",
    "    diff_label_mask = ~same_label_mask & ~eye_mask\n",
    "    \n",
    "    # Compute means\n",
    "    intra_mean = sim_matrix[same_label_mask].mean().item() if same_label_mask.sum() > 0 else 0\n",
    "    inter_mean = sim_matrix[diff_label_mask].mean().item() if diff_label_mask.sum() > 0 else 0\n",
    "    margin = intra_mean - inter_mean\n",
    "    \n",
    "    # Health checks\n",
    "    is_healthy = True\n",
    "    warnings = []\n",
    "    \n",
    "    if emb_std < 0.01:\n",
    "        warnings.append(\"‚ö†Ô∏è  Collapse (std < 0.01)\")\n",
    "        is_healthy = False\n",
    "    \n",
    "    if intra_mean <= inter_mean:\n",
    "        warnings.append(\"‚ö†Ô∏è  Intra ‚â§ Inter\")\n",
    "        is_healthy = False\n",
    "    \n",
    "    if margin < 0.2:\n",
    "        warnings.append(\"‚ö†Ô∏è  Margin < 0.2\")\n",
    "    \n",
    "    # Display\n",
    "    print(f\"  Std: {emb_std:.4f}  |  Intra: {intra_mean:.4f}  |  Inter: {inter_mean:.4f}  |  Margin: {margin:.4f}\")\n",
    "    \n",
    "    if warnings:\n",
    "        for w in warnings:\n",
    "            print(f\"  {w}\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Healthy\")\n",
    "    \n",
    "    model.train()\n",
    "    return {\n",
    "        'std': emb_std,\n",
    "        'intra': intra_mean,\n",
    "        'inter': inter_mean,\n",
    "        'margin': margin,\n",
    "        'healthy': is_healthy\n",
    "    }\n",
    "\n",
    "print(\"‚úì Health monitor ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING LOOP WITH HEALTH MONITORING\n",
    "# ============================================================================\n",
    "print(\"\\nüöÄ Training with Health Monitoring\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "history = {'embedding_std': [], 'intra_sim': [], 'inter_sim': [], 'margin': []}\n",
    "    train_losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    \n",
    "    for images, labels in progress_bar:\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        # Extract Embeddings\n        embeddings = model(images)\n        # Handle tuple return if BAM is on\n        if isinstance(embeddings, tuple):\n             embeddings = embeddings[0]\n             \n        # ArcFace Forward\n        # We need the arcface_head initialized outside\n        thetas = arcface_head(embeddings, labels)\n        \n        # Cross Entropy on ArcFace logits\n        loss = criterion(thetas, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS} - Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # CHECK HEALTH EVERY 5 EPOCHS (reduces CPU overhead)\n",
    "    if (epoch + 1) % 5 == 0 or (epoch + 1) == NUM_EPOCHS:\n",
    "        health = check_embedding_health(model, val_loader, device, epoch+1)\n",
    "        \n",
    "            history['embedding_std'].append(health['std'])\n",
    "        history['intra_sim'].append(health['intra'])\n",
    "        history['inter_sim'].append(health['inter'])\n",
    "        history['margin'].append(health['margin'])\n",
    "\n        # Early stop check (inside health block)\n",
    "        if epoch > 5 and health['std'] < 0.005:\n",
    "            print('‚ùå STOPPING: Severe collapse!')\n",
    "            break\n",
    "    \n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Early stop if collapse\n",
    "print(\"\\n‚úÖ Training Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL EMBEDDING HEALTH REPORT\n",
    "# ============================================================================\n",
    "print(\"\\nüìä FINAL HEALTH REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "final_std = history['embedding_std'][-1]\n",
    "final_margin = history['margin'][-1]\n",
    "final_intra = history['intra_sim'][-1]\n",
    "final_inter = history['inter_sim'][-1]\n",
    "\n",
    "print(f\"\\nFinal: Std={final_std:.4f}, Margin={final_margin:.4f}\")\n",
    "print(f\"       Intra={final_intra:.4f}, Inter={final_inter:.4f}\")\n",
    "\n",
    "print(f\"\\nStd Progression:\")\n",
    "for i, std in enumerate(history['embedding_std']):\n",
    "    status = \"‚úÖ\" if std > 0.01 else \"‚ùå\"\n",
    "    print(f\"  Epoch {i+1}: {std:.4f} {status}\")\n",
    "\n",
    "# Verdict\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "success = (final_std > 0.01 and final_intra > final_inter)\n",
    "\n",
    "if success:\n",
    "    print(\"‚úÖ HEALTHY - Continue to 80-100 epochs!\")\n",
    "    if final_margin >= 0.2:\n",
    "        print(\"‚úÖ Strong discrimination (margin ‚â• 0.2)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Margin < 0.2 (but still discriminating)\")\n",
    "else:\n",
    "    print(\"‚ùå ISSUES DETECTED\")\n",
    "    if final_std <= 0.01:\n",
    "        print(\"  - Embedding collapse\")\n",
    "    if final_intra <= final_inter:\n",
    "        print(\"  - Not discriminating\")\n",
    "    print(\"\\nDebug: Reduce LR to 5e-5 or 3e-5\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss', linewidth=2, color='blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('‚úì Training curve plotted')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done! üéâ\n",
    "\n",
    "### Download from Output tab:\n",
    "- `outputs/models/best_model.pth` - Best trained model\n",
    "- `outputs/models/latest_checkpoint.pth` - Latest checkpoint (for resuming)\n",
    "- `outputs/training_curves.png` - Loss curves\n",
    "- `outputs/training_log.json` - Training metrics\n",
    "- `outputs/visualizations/` - Attention map visualizations\n",
    "\n",
    "### To resume training later:\n",
    "1. Upload the checkpoint file to a new Kaggle notebook\n",
    "2. Set the path in the \"Setup Training\" cell\n",
    "3. Run and confirm to resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Final Evaluation & Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Extract Embeddings\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "print('Extracting embeddings...')\n",
    "embeddings_list = []\n",
    "labels_list = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(val_loader, desc='Extracting'):\n",
    "        images = images.to(device)\n",
    "        emb = model(images)\n",
    "        embeddings_list.append(emb.cpu().numpy())\n",
    "        labels_list.append(labels.cpu().numpy())\n",
    "\n",
    "embeddings = np.vstack(embeddings_list)\n",
    "labels = np.concatenate(labels_list)\n",
    "\n",
    "print(f'‚úì Extracted {len(embeddings)} embeddings from {len(np.unique(labels))} identities')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Compute Metrics\n",
    "def compute_similarity_matrix(embeddings):\n",
    "    return np.dot(embeddings, embeddings.T)\n",
    "\n",
    "def evaluate_ranking(similarity, labels):\n",
    "    n = len(labels)\n",
    "    ranks = []\n",
    "    aps = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        scores = similarity[i].copy()\n",
    "        scores[i] = -np.inf\n",
    "        \n",
    "        gt_mask = (labels == labels[i])\n",
    "        gt_mask[i] = False\n",
    "        \n",
    "        sorted_indices = np.argsort(scores)[::-1]\n",
    "        correct_ranks = np.where(gt_mask[sorted_indices])[0]\n",
    "        \n",
    "        if len(correct_ranks) > 0:\n",
    "            ranks.append(correct_ranks[0] + 1)\n",
    "            num_correct = len(correct_ranks)\n",
    "            precisions = [(k + 1) / (correct_ranks[k] + 1) for k in range(num_correct)]\n",
    "            aps.append(np.mean(precisions))\n",
    "    \n",
    "    rank1 = np.mean(np.array(ranks) == 1) * 100\n",
    "    rank5 = np.mean(np.array(ranks) <= 5) * 100\n",
    "    mAP = np.mean(aps) * 100\n",
    "    \n",
    "    return rank1, rank5, mAP\n",
    "\n",
    "similarity_matrix = compute_similarity_matrix(embeddings)\n",
    "rank1, rank5, mAP = evaluate_ranking(similarity_matrix, labels)\n",
    "\n",
    "print(f'üìä RANKING METRICS:')\n",
    "print(f'   Rank-1: {rank1:.2f}%')\n",
    "print(f'   Rank-5: {rank5:.2f}%')\n",
    "print(f'   mAP:    {mAP:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Similarity Histogram\n",
    "intra_sim = []\n",
    "inter_sim = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    for j in range(i + 1, len(labels)):\n",
    "        sim = similarity_matrix[i, j]\n",
    "        if labels[i] == labels[j]:\n",
    "            intra_sim.append(sim)\n",
    "        else:\n",
    "            inter_sim.append(sim)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(inter_sim, bins=50, alpha=0.6, label='Inter-class', color='red', density=True)\n",
    "plt.hist(intra_sim, bins=50, alpha=0.6, label='Intra-class', color='green', density=True)\n",
    "plt.xlabel('Cosine Similarity', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.title('Similarity Distribution', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('similarity_histogram.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'Intra: {np.mean(intra_sim):.4f}, Inter: {np.mean(inter_sim):.4f}, Margin: {np.mean(intra_sim) - np.mean(inter_sim):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: ROC Curve\n",
    "y_true = []\n",
    "y_scores = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    for j in range(i + 1, len(labels)):\n",
    "        y_true.append(1 if labels[i] == labels[j] else 0)\n",
    "        y_scores.append(similarity_matrix[i, j])\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve - Verification', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'ROC AUC: {roc_auc:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: t-SNE Visualization\n",
    "print('Computing t-SNE (1-2 min)...')\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings)//4))\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "unique_labels = np.unique(labels)\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "for idx, label in enumerate(unique_labels):\n",
    "    mask = labels == label\n",
    "    plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n",
    "                c=[colors[idx]], label=f'ID {label}', s=50, alpha=0.7,\n",
    "                edgecolors='black', linewidth=0.5)\n",
    "\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "plt.title(f't-SNE Embedding ({len(unique_labels)} Elephant IDs)', fontsize=14, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8, ncol=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('tsne_clusters.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('‚úì t-SNE complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Final Summary\n",
    "print('='*70)\n",
    "print('FINAL RESULTS SUMMARY')\n",
    "print('='*70)\n",
    "print(f'\\nüìä Ranking Performance:')\n",
    "print(f'   Rank-1:  {rank1:.2f}%')\n",
    "print(f'   Rank-5:  {rank5:.2f}%')\n",
    "print(f'   mAP:     {mAP:.2f}%')\n",
    "print(f'\\nüìà Verification:')\n",
    "print(f'   ROC AUC: {roc_auc:.3f}')\n",
    "print(f'\\nüìê Embedding Geometry:')\n",
    "print(f'   Intra:   {np.mean(intra_sim):.4f}')\n",
    "print(f'   Inter:   {np.mean(inter_sim):.4f}')\n",
    "print(f'   Margin:  {np.mean(intra_sim) - np.mean(inter_sim):.4f}')\n",
    "print('\\n‚úÖ All visualizations generated!')\n",
    "print('='*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Production Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 1. SAVE FINAL TRAINED MODEL\n",
    "# ==============================\n",
    "\n",
    "save_path = 'makhna_model.pth'\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'num_classes': train_dataset.num_classes,\n",
    "    'identity_to_idx': train_dataset.identity_to_idx\n",
    "}, save_path)\n",
    "\n",
    "print(f'‚úÖ Model saved to {save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 2. GENERATE GALLERY EMBEDDINGS (ALL 19 MAKHNA ELEPHANTS)\n",
    "# ==============================\n",
    "\n",
    "print('Creating FULL dataset loader for gallery (all 19 Makhna elephants)...')\n",
    "\n",
    "# Create evaluation transform (NO ToPILImage since Image.open returns PIL)\n",
    "from torchvision import transforms\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Full dataset class\n",
    "class FullDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.identity_to_idx = {}\n",
    "        self._load_all_images()\n",
    "    \n",
    "    def _load_all_images(self):\n",
    "        idx = 0\n",
    "        for elephant_dir in sorted(self.root_dir.iterdir()):\n",
    "            if not elephant_dir.is_dir():\n",
    "                continue\n",
    "            \n",
    "            elephant_name = elephant_dir.name\n",
    "            self.identity_to_idx[elephant_name] = idx\n",
    "            \n",
    "            for img_path in elephant_dir.glob('*.jpg'):\n",
    "                self.samples.append((img_path, idx))\n",
    "            \n",
    "            idx += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # Returns PIL Image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply transform (Resize, ToTensor, Normalize)\n",
    "        return image, label\n",
    "\n",
    "# Create full dataset\n",
    "full_dataset = FullDataset(root_dir=DATA_ROOT, transform=eval_transform)\n",
    "full_loader = DataLoader(full_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f'‚úÖ Full dataset created:')\n",
    "print(f'   - Total images: {len(full_dataset)}')\n",
    "print(f'   - Total identities: {len(full_dataset.identity_to_idx)}')\n",
    "\n",
    "# Generate embeddings\n",
    "model.eval()\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(full_loader, desc='Generating full gallery'):\n",
    "        images = images.to(device)\n",
    "        embeddings = model(images)\n",
    "        \n",
    "        all_embeddings.append(embeddings.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "# Save gallery\n",
    "gallery_embeddings = torch.cat(all_embeddings)\n",
    "gallery_labels = torch.cat(all_labels)\n",
    "\n",
    "torch.save({\n",
    "    'embeddings': gallery_embeddings,\n",
    "    'labels': gallery_labels,\n",
    "    'idx_to_identity': {v: k for k, v in full_dataset.identity_to_idx.items()}\n",
    "}, 'gallery_embeddings.pt')\n",
    "\n",
    "print(f'\\n‚úÖ Gallery saved: gallery_embeddings.pt')\n",
    "print(f'   - Embeddings: {len(gallery_embeddings)} from {len(torch.unique(gallery_labels))} IDs')\n",
    "print(f'   - Expected: 208 embeddings from 19 Makhna elephants')\n",
    "print('\\n‚ö†Ô∏è  DOWNLOAD gallery_embeddings.pt from Kaggle!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 3. INFERENCE FUNCTION (FIXED)\n",
    "# ==============================\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# CRITICAL: Must match training preprocessing exactly\n",
    "inference_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ADDED\n",
    "])\n",
    "\n",
    "def extract_embedding(image_path):\n",
    "    \"\"\"Extract embedding from image path\"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = inference_transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embedding = model(image)  # Returns normalized embedding\n",
    "    \n",
    "    return embedding.cpu()\n",
    "\n",
    "def match_image(image_path, top_k=5, threshold=0.4):\n",
    "    \"\"\"Find top-k matches for query image with confidence threshold\"\"\"\n",
    "    # Extract query embedding\n",
    "    query_embedding = extract_embedding(image_path)\n",
    "    \n",
    "    # Load gallery\n",
    "    gallery_data = torch.load('gallery_embeddings.pt')\n",
    "    gallery_embeddings = gallery_data['embeddings']\n",
    "    gallery_labels = gallery_data['labels']\n",
    "    idx_to_identity = gallery_data['idx_to_identity']\n",
    "    \n",
    "    # Cosine similarity (embeddings already normalized)\n",
    "    sims = torch.matmul(gallery_embeddings, query_embedding.T).squeeze()\n",
    "    \n",
    "    # Get top-k\n",
    "    topk = torch.topk(sims, min(top_k, len(sims)))\n",
    "    \n",
    "    # Check if top match meets threshold\n",
    "    if topk.values[0] < threshold:\n",
    "        print('\\n‚ö†Ô∏è  UNKNOWN ELEPHANT (confidence too low)')\n",
    "        print(f'   Top similarity: {topk.values[0].item():.4f} < {threshold}')\n",
    "        return None\n",
    "    \n",
    "    print('\\nüîç Top Matches:')\n",
    "    print('-' * 50)\n",
    "    for rank, (score, idx) in enumerate(zip(topk.values, topk.indices), 1):\n",
    "        identity = idx_to_identity[gallery_labels[idx].item()]\n",
    "        confidence = '‚úì' if score >= threshold else '‚úó'\n",
    "        print(f'  {rank}. {identity:20s} | Similarity: {score.item():.4f} {confidence}')\n",
    "    print('-' * 50)\n",
    "    \n",
    "    return topk\n",
    "\n",
    "print('‚úÖ Inference functions defined (FIXED)')\n",
    "print('\\nImprovements:')\n",
    "print('  ‚úì Added ImageNet normalization (matches training)')\n",
    "print('  ‚úì Added confidence threshold (default 0.4)')\n",
    "print('  ‚úì Returns None for unknown elephants')\n",
    "print('\\nUsage:')\n",
    "print('  match_image(\"path/to/elephant.jpg\", top_k=5, threshold=0.4)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Inference (Optional)\n",
    "\n",
    "Uncomment and run to test the inference function on a sample image:\n",
    "\n",
    "```python\n",
    "# Example: Test on a validation image\n",
    "# test_image = list(Path(DATA_ROOT).rglob('*.jpg'))[0]\n",
    "# match_image(str(test_image), top_k=5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# FINAL EVALUATION SUMMARY\n",
    "# ==============================\n",
    "\n",
    "print('='*70)\n",
    "print('FINAL EVALUATION RESULTS - MAKHNA BIOMETRIC SYSTEM')\n",
    "print('='*70)\n",
    "\n",
    "print('\\nüìä RANKING PERFORMANCE:')\n",
    "print(f'   Rank-1 Accuracy:  {rank1:.2f}%')\n",
    "print(f'   Rank-5 Accuracy:  {rank5:.2f}%')\n",
    "print(f'   mAP:              {mAP:.2f}%')\n",
    "\n",
    "print('\\nüìà VERIFICATION PERFORMANCE:')\n",
    "print(f'   ROC AUC:          {roc_auc:.3f}')\n",
    "\n",
    "print('\\nüìê EMBEDDING GEOMETRY:')\n",
    "print(f'   Intra Similarity: {np.mean(intra_sim):.4f}')\n",
    "print(f'   Inter Similarity: {np.mean(inter_sim):.4f}')\n",
    "print(f'   Margin:           {np.mean(intra_sim) - np.mean(inter_sim):.4f}')\n",
    "print(f'   Embedding Std:    {np.std(embeddings):.4f}')\n",
    "\n",
    "print('\\nüíæ DATASET:')\n",
    "print(f'   Identities:       {len(np.unique(labels))}')\n",
    "print(f'   Total Samples:    {len(embeddings)}')\n",
    "print(f'   Avg Images/ID:    {len(embeddings) / len(np.unique(labels)):.1f}')\n",
    "\n",
    "print('\\n‚úÖ PRODUCTION ARTIFACTS:')\n",
    "print('   - makhna_model.pth (trained model + config)')\n",
    "print('   - gallery_embeddings.pt (precomputed embeddings)')\n",
    "print('   - similarity_histogram.png')\n",
    "print('   - roc_curve.png')\n",
    "print('   - tsne_clusters.png')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('SYSTEM READY FOR DEPLOYMENT')\n",
    "print('='*70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}